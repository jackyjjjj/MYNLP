{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 23:40:26.993900: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 23:40:27.011808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742226027.031197    1708 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742226027.037095    1708 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742226027.052645    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742226027.052659    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742226027.052661    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742226027.052663    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 23:40:27.057394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, \\\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataPath = r'/root/autodl-tmp/MyData/process/Bert/baidu-api'\n",
    "savePath = r'/root/autodl-tmp/MyData/process/Bert/train'\n",
    "modelSavePath = r'/root/autodl-tmp/MyData/process/Bert/bert-trained'\n",
    "\n",
    "fileName = 'comments.xlsx'\n",
    "excel_file_path = os.path.join(dataPath, fileName)\n",
    "\n",
    "df = pd.read_excel(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/autodl-tmp/MyData/process/Bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2. æ•°æ®é¢„å¤„ç†\n",
    "# å‡è®¾ DataFrame ä¸­æœ‰ 'text' å’Œ 'label' ä¸¤åˆ—\n",
    "df = df[['comment', 'label']]\n",
    "\n",
    "# 3. åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)  # 80% è®­ç»ƒï¼Œ20% æµ‹è¯•\n",
    "\n",
    "# 4. å°† DataFrame è½¬æ¢ä¸º Hugging Face æ•°æ®é›†\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# 5. åŠ è½½ä¸­æ–‡ BERT tokenizer å’Œæ¨¡å‹\n",
    "tokenizer = BertTokenizer.from_pretrained(r'/root/autodl-tmp/MyData/process/Bert/bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained(r'/root/autodl-tmp/MyData/process/Bert/bert-base-chinese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4a4b656a6c42bb964c26d15c085488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70d8a7c35e9420ea649e663fabf1435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['comment'], truncation=True)\n",
    "\n",
    "# 7. å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œåˆ†è¯\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 8. è®¾ç½®æ•°æ®æ ¼å¼\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# é…ç½®è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../bert-trained/results',  # è¾“å‡ºç›®å½•\n",
    "    evaluation_strategy=\"epoch\",  # æ¯ä¸ª epoch åè¯„ä¼°ä¸€æ¬¡\n",
    "    learning_rate=2e-5,  # åˆå§‹å­¦ä¹ ç‡\n",
    "    per_device_train_batch_size=16,  # æ¯ä¸ªè®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size=64,  # æ¯ä¸ªè®¾å¤‡è¯„ä¼°æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs=10,  # è®­ç»ƒ epoch æ•°\n",
    "    weight_decay=0.01,  # æƒé‡è¡°å‡\n",
    "    lr_scheduler_type='linear',  # ä½¿ç”¨çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    warmup_steps=500,  # é¢„çƒ­æ­¥æ•°ï¼Œå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡\n",
    "    logging_dir='./logs',  # æ—¥å¿—ä¿å­˜è·¯å¾„\n",
    "    logging_steps=10,  # æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "    save_strategy=\"epoch\",  # æ¯ä¸ª epoch åä¿å­˜æ¨¡å‹\n",
    "    load_best_model_at_end=True,  # åŠ è½½éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # ç›‘æ§éªŒè¯é›†ä¸Šçš„æŸå¤±\n",
    "    greater_is_better=False,  # æŸå¤±è¶Šå°è¶Šå¥½\n",
    ")\n",
    "\n",
    "# å¦‚æœéœ€è¦è‡ªå®šä¹‰ä¼˜åŒ–å™¨ AdamWï¼ˆé»˜è®¤çš„ï¼‰ï¼Œå¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è®¾ç½®\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "# è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "num_training_steps = len(train_dataset) * training_args.num_train_epochs // training_args.per_device_train_batch_size\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# æ·»åŠ æå‰åœæ­¢å›è°ƒ\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)  # 3 ä¸ª epoch æ²¡æœ‰æå‡å°±åœæ­¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ä½¿ç”¨ Trainer è¿›è¡Œè®­ç»ƒ\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ä½ è®­ç»ƒçš„æ¨¡å‹\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# è®­ç»ƒé…ç½®\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ä¼˜åŒ–å™¨\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# å­¦ä¹ ç‡è°ƒåº¦å™¨\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# è®­ç»ƒæ•°æ®\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# éªŒè¯æ•°æ®\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# æ·»åŠ æå‰åœæ­¢å›è°ƒ\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ Trainer è¿›è¡Œè®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model=model,  # ä½ è®­ç»ƒçš„æ¨¡å‹\n",
    "    args=training_args,  # è®­ç»ƒé…ç½®\n",
    "    optimizer=optimizer,  # ä¼˜åŒ–å™¨\n",
    "    lr_scheduler=lr_scheduler,  # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    train_dataset=train_dataset,  # è®­ç»ƒæ•°æ®\n",
    "    eval_dataset=test_dataset,  # éªŒè¯æ•°æ®\n",
    "    callbacks=[early_stopping_callback]  # æ·»åŠ æå‰åœæ­¢å›è°ƒ\n",
    ")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "train_result = trainer.train()\n",
    "\n",
    "# 12. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
    "model.save_pretrained(os.path.join(modelSavePath, \"./bert_trained_by_20000\"))\n",
    "tokenizer.save_pretrained(os.path.join(modelSavePath, \"./bert_trained_by_20000\"))\n",
    "\n",
    "# 13. è¯„ä¼°æ¨¡å‹\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# 14. æ‰“å°è¯„ä¼°ç»“æœ\n",
    "print(f\"è¯„ä¼°ç»“æœ: {eval_results}\")\n",
    "\n",
    "# 15. è®¡ç®—å‡†ç¡®åº¦\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# è®¡ç®—å‡†ç¡®åº¦\n",
    "accuracy = (preds == labels).mean()\n",
    "print(f\"æ¨¡å‹å‡†ç¡®åº¦: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 16. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "# æå–è®­ç»ƒå’ŒéªŒè¯æŸå¤±\n",
    "train_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(range(1, len(eval_loss) + 1), eval_loss, label='Evaluation Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# æå–å­¦ä¹ ç‡\n",
    "learning_rates = [log['learning_rate'] for log in trainer.state.log_history if 'learning_rate' in log]\n",
    "steps = range(1, len(learning_rates) + 1)\n",
    "\n",
    "# ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿\n",
    "plt.plot(steps, learning_rates)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
