{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 23:40:26.993900: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 23:40:27.011808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742226027.031197    1708 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742226027.037095    1708 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742226027.052645    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742226027.052659    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742226027.052661    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742226027.052663    1708 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 23:40:27.057394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, \\\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataPath = r'/root/autodl-tmp/MyData/process/Bert/baidu-api'\n",
    "savePath = r'/root/autodl-tmp/MyData/process/Bert/train'\n",
    "modelSavePath = r'/root/autodl-tmp/MyData/process/Bert/bert-trained'\n",
    "\n",
    "fileName = 'comments.xlsx'\n",
    "excel_file_path = os.path.join(dataPath, fileName)\n",
    "\n",
    "df = pd.read_excel(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/autodl-tmp/MyData/process/Bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2. 数据预处理\n",
    "# 假设 DataFrame 中有 'text' 和 'label' 两列\n",
    "df = df[['comment', 'label']]\n",
    "\n",
    "# 3. 切分训练集和验证集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)  # 80% 训练，20% 测试\n",
    "\n",
    "# 4. 将 DataFrame 转换为 Hugging Face 数据集\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# 5. 加载中文 BERT tokenizer 和模型\n",
    "tokenizer = BertTokenizer.from_pretrained(r'/root/autodl-tmp/MyData/process/Bert/bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained(r'/root/autodl-tmp/MyData/process/Bert/bert-base-chinese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4a4b656a6c42bb964c26d15c085488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70d8a7c35e9420ea649e663fabf1435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. 数据预处理函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['comment'], truncation=True)\n",
    "\n",
    "# 7. 对训练集和验证集进行分词\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 8. 设置数据格式\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 配置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../bert-trained/results',  # 输出目录\n",
    "    evaluation_strategy=\"epoch\",  # 每个 epoch 后评估一次\n",
    "    learning_rate=2e-5,  # 初始学习率\n",
    "    per_device_train_batch_size=16,  # 每个设备训练批次大小\n",
    "    per_device_eval_batch_size=64,  # 每个设备评估批次大小\n",
    "    num_train_epochs=10,  # 训练 epoch 数\n",
    "    weight_decay=0.01,  # 权重衰减\n",
    "    lr_scheduler_type='linear',  # 使用线性学习率调度器\n",
    "    warmup_steps=500,  # 预热步数，学习率从 0 线性增加到初始学习率\n",
    "    logging_dir='./logs',  # 日志保存路径\n",
    "    logging_steps=10,  # 每 10 步记录一次日志\n",
    "    save_strategy=\"epoch\",  # 每个 epoch 后保存模型\n",
    "    load_best_model_at_end=True,  # 加载验证集上表现最好的模型\n",
    "    metric_for_best_model=\"eval_loss\",  # 监控验证集上的损失\n",
    "    greater_is_better=False,  # 损失越小越好\n",
    ")\n",
    "\n",
    "# 如果需要自定义优化器 AdamW（默认的），可以按如下方式设置\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "# 获取学习率调度器\n",
    "num_training_steps = len(train_dataset) * training_args.num_train_epochs // training_args.per_device_train_batch_size\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 添加提前停止回调\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)  # 3 个 epoch 没有提升就停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 使用 Trainer 进行训练\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 你训练的模型\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 训练配置\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 优化器\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 学习率调度器\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 训练数据\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 验证数据\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 添加提前停止回调\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "# 使用 Trainer 进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,  # 你训练的模型\n",
    "    args=training_args,  # 训练配置\n",
    "    optimizer=optimizer,  # 优化器\n",
    "    lr_scheduler=lr_scheduler,  # 学习率调度器\n",
    "    train_dataset=train_dataset,  # 训练数据\n",
    "    eval_dataset=test_dataset,  # 验证数据\n",
    "    callbacks=[early_stopping_callback]  # 添加提前停止回调\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "train_result = trainer.train()\n",
    "\n",
    "# 12. 保存微调后的模型\n",
    "model.save_pretrained(os.path.join(modelSavePath, \"./bert_trained_by_20000\"))\n",
    "tokenizer.save_pretrained(os.path.join(modelSavePath, \"./bert_trained_by_20000\"))\n",
    "\n",
    "# 13. 评估模型\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# 14. 打印评估结果\n",
    "print(f\"评估结果: {eval_results}\")\n",
    "\n",
    "# 15. 计算准确度\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# 计算准确度\n",
    "accuracy = (preds == labels).mean()\n",
    "print(f\"模型准确度: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 16. 可视化训练过程\n",
    "# 提取训练和验证损失\n",
    "train_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(range(1, len(eval_loss) + 1), eval_loss, label='Evaluation Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 提取学习率\n",
    "learning_rates = [log['learning_rate'] for log in trainer.state.log_history if 'learning_rate' in log]\n",
    "steps = range(1, len(learning_rates) + 1)\n",
    "\n",
    "# 绘制学习率曲线\n",
    "plt.plot(steps, learning_rates)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
